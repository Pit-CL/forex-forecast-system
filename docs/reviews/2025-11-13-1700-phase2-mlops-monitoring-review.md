# Code Review: Phase 2 MLOps Monitoring Implementation

**Fecha:** 2025-11-13 17:00
**Revisor:** Code Reviewer Agent
**Archivos revisados:**
- `src/forex_core/mlops/drift_trends.py` (486 lines)
- `src/forex_core/mlops/validation.py` (547 lines)
- `src/forex_core/mlops/dashboard_utils.py` (564 lines)
- `scripts/mlops_dashboard.py` (432 lines)
- `scripts/validate_model.py` (307 lines)
- `src/forex_core/mlops/monitoring.py` (modifications)
- `src/forex_core/mlops/__init__.py` (modifications)

**Complejidad del cambio:** Complejo
**Total de lineas:** 2,020+ lines across 9 files

---

## ‚ö° TL;DR (Resumen Ejecutivo)

**Veredicto General:** üü° Aprobado con recomendaciones importantes

**Impacto del cambio:** Alto - Sistema de monitoreo completo para produccion

**Principales hallazgos:**
- üü¢ Arquitectura bien estructurada con separacion de responsabilidades clara
- üü¢ Documentacion exhaustiva y type hints completos
- üü° Storage concurrency no manejado (writes simultaneos pueden corromper Parquet)
- üü° Drift scoring weights hardcoded - deben ser configurables
- üü° Validacion crea DataBundle mock - potencialmente incorrecta en produccion
- üî¥ Path traversal risk en dashboard_utils (acepta user input sin sanitizacion)
- üî¥ Resource exhaustion possible con series grandes (sin limites de memoria)

**Accion recomendada:** Corregir issues criticos de seguridad y concurrencia antes de uso intensivo

---

## üìä Metricas del Codigo

| Metrica | Valor | Status |
|---------|-------|--------|
| Archivos nuevos | 5 | ‚ÑπÔ∏è |
| Archivos modificados | 2 | ‚ÑπÔ∏è |
| Lineas anadidas | +2,020 | ‚ÑπÔ∏è |
| Complejidad ciclomatica (max) | ~15 | üü° |
| Funciones >30 lineas | ~12 | üü° |
| Type hints coverage | ~98% | üü¢ |
| Docstring coverage | ~95% | üü¢ |
| Test coverage (estimado) | ~15% | üî¥ |

---

## üîç Analisis Detallado

### 1. Arquitectura y Diseno [üü¢]

#### ‚úÖ Aspectos Positivos:

**Excelente separacion de responsabilidades:**
- `drift_trends.py`: Solo analisis de tendencias
- `validation.py`: Solo validacion walk-forward
- `dashboard_utils.py`: Solo utilidades de agregacion
- `mlops_dashboard.py`: Solo presentacion CLI
- Cada modulo tiene una unica responsabilidad clara

**Uso correcto de dataclasses:**
```python
@dataclass
class DriftTrendReport:
    trend: DriftTrend
    current_score: float
    ...

    def requires_action(self) -> bool:  # Business logic en el modelo
        return (
            self.trend == DriftTrend.WORSENING
            or self.consecutive_high >= 3
            or self.current_score >= 75
        )
```
‚úÖ Combina datos + comportamiento relacionado

**Enums bien usados:**
```python
class DriftTrend(str, Enum):
    STABLE = "stable"
    IMPROVING = "improving"
    WORSENING = "worsening"
    UNKNOWN = "unknown"
```
‚úÖ `str, Enum` permite comparacion directa con strings y serializacion JSON

**Dependency injection apropiada:**
```python
def __init__(
    self,
    storage_path: Optional[Path] = None,
    drift_detector: Optional[DataDriftDetector] = None,
):
```
‚úÖ Permite testing facil con mocks

#### üî¥ Issues Criticos:

**CRIT-1: Parquet concurrency no manejada**

**Archivo:** `drift_trends.py:139-146`, `validation.py:527-534`

**Problema:**
```python
# En record_drift()
if self.storage_path.exists():
    df = pd.read_parquet(self.storage_path)
    df = pd.concat([df, pd.DataFrame([record])], ignore_index=True)
else:
    df = pd.DataFrame([record])

df.to_parquet(self.storage_path, index=False)
```

Race condition:
1. Process A lee archivo
2. Process B lee archivo (mismo estado)
3. Process A escribe nuevos datos
4. Process B escribe, sobrescribiendo cambios de A ‚ùå

**Impacto:** Perdida de datos si multiples procesos escriben simultaneamente

**Solucion sugerida:**
```python
import fcntl  # Para file locking en Unix
from contextlib import contextmanager

@contextmanager
def locked_parquet_write(filepath):
    """Context manager for safe concurrent writes."""
    lock_file = filepath.parent / f"{filepath.name}.lock"

    with open(lock_file, 'w') as lock:
        try:
            fcntl.flock(lock.fileno(), fcntl.LOCK_EX)  # Exclusive lock
            yield
        finally:
            fcntl.flock(lock.fileno(), fcntl.LOCK_UN)

def record_drift(self, report: DriftReport, horizon: str) -> None:
    with locked_parquet_write(self.storage_path):
        if self.storage_path.exists():
            df = pd.read_parquet(self.storage_path)
            df = pd.concat([df, pd.DataFrame([record])], ignore_index=True)
        else:
            df = pd.DataFrame([record])

        # Atomic write: write to temp, then rename
        temp_path = self.storage_path.with_suffix('.tmp')
        df.to_parquet(temp_path, index=False)
        temp_path.replace(self.storage_path)  # Atomic on POSIX
```

**Razon:** Previene corruption de datos en escrituras concurrentes

**Alternativa:** Usar base de datos (SQLite) en lugar de Parquet para concurrencia:
```python
# SQLite con WAL mode permite lecturas concurrentes + 1 escritor
import sqlite3

conn = sqlite3.connect('drift_history.db')
conn.execute("PRAGMA journal_mode=WAL")
conn.execute("""
    CREATE TABLE IF NOT EXISTS drift_history (
        timestamp TEXT,
        horizon TEXT,
        drift_score REAL,
        ...
    )
""")
conn.execute("INSERT INTO drift_history VALUES (?, ?, ...)", record)
conn.commit()
```

#### üü° Sugerencias de Mejora:

**Sugerencia #1: Configurar drift score weights**

**Archivo:** `drift_trends.py:253-295`

**Actual:**
```python
def _calculate_drift_score(self, report: DriftReport) -> float:
    ks_score = min(report.statistic * 100, 100) * 0.40
    t_score = max(0, (1 - t_test.p_value) * 100) * 0.25
    levene_score = max(0, (1 - levene_test.p_value) * 100) * 0.20
    ljungbox_score = max(0, (1 - ljungbox_test.p_value) * 100) * 0.15
```
Weights hardcoded: KS=40%, T=25%, Levene=20%, LB=15%

**Sugerido:**
```python
@dataclass
class DriftScoreWeights:
    """Configurable weights for drift score calculation."""
    ks_weight: float = 0.40
    t_test_weight: float = 0.25
    levene_weight: float = 0.20
    ljungbox_weight: float = 0.15

    def __post_init__(self):
        total = sum([self.ks_weight, self.t_test_weight,
                     self.levene_weight, self.ljungbox_weight])
        if not np.isclose(total, 1.0):
            raise ValueError(f"Weights must sum to 1.0, got {total}")

class DriftTrendAnalyzer:
    def __init__(
        self,
        storage_path: Optional[Path] = None,
        drift_detector: Optional[DataDriftDetector] = None,
        score_weights: Optional[DriftScoreWeights] = None,
    ):
        self.score_weights = score_weights or DriftScoreWeights()

    def _calculate_drift_score(self, report: DriftReport) -> float:
        ks_score = min(report.statistic * 100, 100) * self.score_weights.ks_weight
        t_score = max(0, (1 - t_test.p_value) * 100) * self.score_weights.t_test_weight
        # ... etc
```

**Beneficio:**
- Permite A/B testing de diferentes weighing strategies
- Configurable via settings en produccion
- Mas facil experimentar con mejores formulas

---

### 2. Legibilidad y Mantenibilidad [üü¢]

#### ‚úÖ Aspectos Positivos:

**Docstrings excelentes:**
```python
def detect_mean_shift(self, series: pd.Series) -> DriftTestResult:
    """
    Detect shift in mean level using Welch's t-test.

    The t-test compares the means of two independent samples to determine
    if they come from populations with equal means. Welch's version doesn't
    assume equal variances, making it more robust.

    Args:
        series: Time series data (indexed by date).

    Returns:
        DriftTestResult with t-test results and interpretation.

    Example:
        >>> result = detector.detect_mean_shift(usdclp_series)
        >>> if result.drift_detected:
        ...     print(f"Mean has shifted: {result.description}")
    """
```
‚úÖ Explica el "por que" (razon de usar Welch's t-test)
‚úÖ Incluye ejemplo de uso
‚úÖ Type hints completos

**Nombres descriptivos y consistentes:**
```python
# Good naming pattern
get_prediction_summary()
get_drift_summary()
get_validation_summary()
get_alert_summary()

# Consistent structure
analyze_trend()
record_drift()
detect_drift()
```
‚úÖ Verbos claros: get, analyze, record, detect
‚úÖ Patron consistente en dashboard_utils

**Funciones pequenas y focalizadas:**
```python
def _count_consecutive_high(self, history: pd.DataFrame) -> int:
    """Cuenta dias consecutivos con severidad HIGH."""
    if history.empty:
        return 0

    recent = history.iloc[::-1]
    count = 0
    for _, row in recent.iterrows():
        if row["severity"] == DriftSeverity.HIGH.value:
            count += 1
        else:
            break

    return count
```
‚úÖ 13 lineas, single responsibility, facil de entender

#### üü° Sugerencias de Mejora:

**Sugerencia #2: Extraer magic numbers a constantes**

**Archivo:** `drift_trends.py:356-372`

**Actual:**
```python
def _detect_trend(self, slope: float, r2: float, current_score: float) -> DriftTrend:
    if r2 < 0.3:  # Magic number
        if current_score < 25:  # Magic number
            return DriftTrend.STABLE
        else:
            return DriftTrend.UNKNOWN

    if abs(slope) < 0.5:  # Magic number
        return DriftTrend.STABLE
```

**Sugerido:**
```python
# At class level or config
R2_THRESHOLD_WEAK_TREND = 0.3  # R¬≤ below this = unclear trend
SCORE_THRESHOLD_STABLE = 25    # Score below this = stable
SLOPE_THRESHOLD_FLAT = 0.5     # Slope below this = flat trend

def _detect_trend(self, slope: float, r2: float, current_score: float) -> DriftTrend:
    # Need strong R¬≤ to claim trend
    if r2 < R2_THRESHOLD_WEAK_TREND:
        if current_score < SCORE_THRESHOLD_STABLE:
            return DriftTrend.STABLE
        else:
            return DriftTrend.UNKNOWN

    if abs(slope) < SLOPE_THRESHOLD_FLAT:
        return DriftTrend.STABLE
```

**Beneficio:** Facil ajustar thresholds sin buscar en codigo

**Sugerencia #3: Reducir complejidad en get_alert_details**

**Archivo:** `dashboard_utils.py:390-497`

**Problema:** Funcion de 107 lineas con parsing complejo de logs

**Actual:**
```python
def get_alert_details(horizon, days, severity) -> Optional[dict]:
    # ... 107 lines of nested loops and parsing ...
    for h in horizons:
        with open(log_file, "r") as f:
            content = f.read()

        for block in content.split("=" * 60):
            if "ALERT EVALUATION" in block:
                try:
                    for line in block.split("\n"):
                        if line.startswith("Timestamp:"):
                            # ... complex parsing ...
```

**Sugerido:**
```python
def _parse_alert_block(block: str, cutoff: datetime) -> Optional[dict]:
    """Parse single alert evaluation block."""
    if "ALERT EVALUATION" not in block:
        return None

    timestamp = None
    decision = None
    severity = None
    reason = None

    for line in block.split("\n"):
        if line.startswith("Timestamp:"):
            timestamp_str = line.split("Timestamp:")[1].strip()
            timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
        elif line.startswith("Decision:"):
            decision = line.split("Decision:")[1].strip()
        elif line.startswith("Severity:"):
            severity = line.split("Severity:")[1].strip()
        elif line.startswith("Reason:"):
            reason = line.split("Reason:")[1].strip()

    if not timestamp or timestamp < cutoff:
        return None

    return {
        "timestamp": timestamp,
        "decision": decision,
        "severity": severity,
        "reason": reason,
    }

def get_alert_details(horizon, days, severity) -> Optional[dict]:
    """Get detailed alert history."""
    logs_dir = Path("logs")
    if not logs_dir.exists():
        return None

    horizons = [horizon] if horizon else ["7d", "15d", "30d", "90d"]
    cutoff = datetime.now() - timedelta(days=days)

    frequency = []
    recent = []

    for h in horizons:
        log_file = logs_dir / f"alerts_{h}.log"
        if not log_file.exists():
            continue

        stats = _process_alert_log(log_file, cutoff, severity)
        frequency.append(stats["frequency"])
        recent.extend(stats["recent"])

    recent.sort(key=lambda x: x["timestamp"], reverse=True)
    return {"frequency": frequency, "recent": recent}
```

**Beneficio:**
- Mas facil de testear (cada funcion pequena)
- Reduce complejidad ciclomatica
- Mas facil de entender

---

### 3. Performance y Eficiencia [üü°]

#### ‚úÖ Aspectos Positivos:

**Uso eficiente de Parquet:**
```python
df = pd.read_parquet(self.storage_path)
```
‚úÖ Parquet es columnar, rapido para analytics
‚úÖ Buena compresion (50-80% vs CSV)

**Ventanas de tiempo apropiadas:**
```python
baseline_window: int = 90  # 3 months
test_window: int = 30      # 1 month
```
‚úÖ Balance entre estabilidad y sensibilidad

#### üî¥ Issues Criticos:

**CRIT-2: Resource exhaustion possible**

**Archivo:** `validation.py:226-288`, `dashboard_utils.py` multiples funciones

**Problema:** No hay limites de memoria para series grandes

```python
def validate(self, series: pd.Series, max_folds: Optional[int] = None):
    # Si series tiene 10 anos de datos diarios (3,650 puntos)
    # Y max_folds=50
    # = 50 forecasts * DataBundle creations = potencialmente GB de RAM

    for i, (train_idx, test_idx) in enumerate(folds, 1):
        bundle = self._create_bundle(train_series)  # Crea bundle completo
        forecast = self.forecaster_func(bundle, self.horizon_days)
```

Sin limites, un usuario puede crashear el sistema con:
```bash
python validate_model.py --horizon 90 --folds 100 --initial-train 3650
```

**Solucion sugerida:**
```python
MAX_VALIDATION_FOLDS = 20
MAX_SERIES_LENGTH = 5000  # ~13 years daily data
MAX_FORECAST_HORIZON = 365

def validate(
    self,
    series: pd.Series,
    max_folds: Optional[int] = None,
) -> ValidationReport:
    # Input validation
    if len(series) > MAX_SERIES_LENGTH:
        logger.warning(
            f"Series too long ({len(series)} > {MAX_SERIES_LENGTH}). "
            f"Truncating to most recent {MAX_SERIES_LENGTH} points."
        )
        series = series.iloc[-MAX_SERIES_LENGTH:]

    if max_folds and max_folds > MAX_VALIDATION_FOLDS:
        logger.warning(
            f"Too many folds requested ({max_folds} > {MAX_VALIDATION_FOLDS}). "
            f"Limiting to {MAX_VALIDATION_FOLDS}."
        )
        max_folds = MAX_VALIDATION_FOLDS

    # ... rest of validation
```

**Archivo de config:**
```python
# forex_core/config.py
class MLOpsSettings:
    MAX_VALIDATION_FOLDS: int = 20
    MAX_SERIES_LENGTH: int = 5000
    MAX_FORECAST_HORIZON: int = 365
    MAX_PARQUET_FILE_SIZE_MB: int = 100
```

#### üü° Sugerencias de Mejora:

**Sugerencia #4: Optimizar lectura repetida de Parquet**

**Archivo:** `dashboard_utils.py:65-116`

**Problema:** Multiples funciones leen mismo archivo repetidamente

```python
def get_drift_summary():
    df = pd.read_parquet(storage_path)  # Read 1
    for horizon in df["horizon"].unique():
        analyzer.analyze_trend(horizon)  # Internally reads again!

def get_drift_details(horizon, days):
    df = pd.read_parquet(storage_path)  # Read 2
```

**Sugerido:** Cache con TTL
```python
from functools import lru_cache
from datetime import datetime, timedelta

class ParquetCache:
    def __init__(self, ttl_seconds: int = 60):
        self.ttl_seconds = ttl_seconds
        self._cache = {}
        self._timestamps = {}

    def get(self, path: Path) -> Optional[pd.DataFrame]:
        if path not in self._cache:
            return None

        age = (datetime.now() - self._timestamps[path]).total_seconds()
        if age > self.ttl_seconds:
            del self._cache[path]
            del self._timestamps[path]
            return None

        return self._cache[path]

    def set(self, path: Path, df: pd.DataFrame):
        self._cache[path] = df
        self._timestamps[path] = datetime.now()

# Global cache for dashboard
_parquet_cache = ParquetCache(ttl_seconds=60)

def get_drift_summary():
    storage_path = ...

    df = _parquet_cache.get(storage_path)
    if df is None:
        df = pd.read_parquet(storage_path)
        _parquet_cache.set(storage_path, df)

    # Use cached df
```

**Beneficio:** -80% lecturas de disco en dashboard

---

### 4. Error Handling y Robustez [üü°]

#### ‚úÖ Aspectos Positivos:

**Try-catch apropiado con logging:**
```python
try:
    metrics = self._execute_fold(fold, series, train_idx, test_idx)
    fold_metrics.append(metrics)
    logger.info(f"Fold {i} complete: RMSE={metrics.rmse:.2f}")
except Exception as e:
    logger.error(f"Fold {i} failed: {e}")
    continue  # Continue with next fold
```
‚úÖ Captura error especifico por fold
‚úÖ Logea error con contexto
‚úÖ No detiene toda la validacion

**Validacion de inputs:**
```python
if history.empty or len(history) < 3:
    logger.warning(f"Insufficient data for trend analysis: {len(history)} records")
    return self._create_empty_report()
```
‚úÖ Fail-fast con mensaje claro
‚úÖ Retorna reporte valido (empty) en lugar de exception

#### üî¥ Issues Criticos:

**CRIT-3: Path traversal vulnerability**

**Archivo:** `dashboard_utils.py:336-387`, `mlops_dashboard.py:219-299`

**Problema:** User input usado directamente para file paths

```python
@app.command()
def validation(
    horizon: str = typer.Option("7d", "--horizon", "-h"),  # User input
    limit: int = typer.Option(5, "--limit", "-n"),
):
    # ...
    val_data = get_validation_details(horizon, limit)

def get_validation_details(horizon: str, limit: int):
    # User-controlled horizon usado en path
    pattern = f"summary_validation_{horizon}_*.parquet"
    summary_files = sorted(reports_dir.glob(pattern))
```

Ataque posible:
```bash
python mlops_dashboard.py validation --horizon "../../etc/passwd"
# Pattern becomes: "summary_validation_../../etc/passwd_*.parquet"
# glob() puede acceder fuera de reports_dir
```

**Solucion sugerida:**
```python
VALID_HORIZONS = {"7d", "15d", "30d", "90d"}

def validate_horizon(horizon: str) -> str:
    """Validate and sanitize horizon parameter."""
    if horizon not in VALID_HORIZONS:
        raise ValueError(
            f"Invalid horizon '{horizon}'. "
            f"Must be one of: {', '.join(VALID_HORIZONS)}"
        )
    return horizon

@app.command()
def validation(
    horizon: str = typer.Option("7d", "--horizon", "-h"),
    limit: int = typer.Option(5, "--limit", "-n"),
):
    try:
        horizon = validate_horizon(horizon)
    except ValueError as e:
        console.print(f"[red]{e}[/red]")
        raise typer.Exit(1)

    val_data = get_validation_details(horizon, limit)
```

**Tambien sanitizar en get_validation_details:**
```python
def get_validation_details(horizon: str, limit: int):
    # Sanitize horizon even if coming from internal call
    horizon = validate_horizon(horizon)

    # Ensure limit is reasonable
    if limit < 1 or limit > 100:
        raise ValueError(f"Limit must be 1-100, got {limit}")

    pattern = f"summary_validation_{horizon}_*.parquet"
    summary_files = sorted(reports_dir.glob(pattern))
```

#### üü° Sugerencias de Mejora:

**Sugerencia #5: Mejor manejo de archivos faltantes**

**Archivo:** `dashboard_utils.py:178-238`

**Actual:**
```python
def get_alert_summary(days: int = 7):
    logs_dir = Path("logs")

    if not logs_dir.exists():
        return []

    for horizon in ["7d", "15d", "30d", "90d"]:
        log_file = logs_dir / f"alerts_{horizon}.log"

        if not log_file.exists():
            continue  # Skip silently
```

**Problema:** Silenciosamente skips missing files. Usuario no sabe si:
- No hay datos
- Archivo no existe (error de configuracion)
- Permiso denegado

**Sugerido:**
```python
def get_alert_summary(days: int = 7):
    logs_dir = Path("logs")

    if not logs_dir.exists():
        logger.warning(f"Logs directory not found: {logs_dir}")
        return []

    summary = []
    missing_files = []

    for horizon in ["7d", "15d", "30d", "90d"]:
        log_file = logs_dir / f"alerts_{horizon}.log"

        if not log_file.exists():
            missing_files.append(horizon)
            continue

        # ... process file

    if missing_files:
        logger.info(
            f"Alert logs not found for horizons: {', '.join(missing_files)}. "
            f"These may not have been generated yet."
        )

    return summary
```

**Beneficio:** Usuario sabe por que no ve datos

---

### 5. Testing y Testabilidad [üî¥]

#### ‚úÖ Aspectos Positivos:

**Codigo testeable con dependency injection:**
```python
class DriftTrendAnalyzer:
    def __init__(
        self,
        storage_path: Optional[Path] = None,
        drift_detector: Optional[DataDriftDetector] = None,
    ):
```
‚úÖ Facil mockear drift_detector en tests
‚úÖ Facil usar temp path para storage en tests

**Funciones puras para logica de negocio:**
```python
def _calculate_drift_score(self, report: DriftReport) -> float:
    # No side effects, deterministic
    ks_score = min(report.statistic * 100, 100) * 0.40
    # ...
    return total_score
```
‚úÖ Facil de testear: input -> output

#### üî¥ Issues Criticos:

**CRIT-4: Tests insuficientes para codigo critico**

**Problema:** Solo hay tests para `monitoring.py` (Phase 1)
- ‚ùå No tests para `drift_trends.py`
- ‚ùå No tests para `validation.py`
- ‚ùå No tests para `dashboard_utils.py`

Codigo critico sin tests:
1. **Drift score calculation** - formula compleja, debe ser correcta
2. **Fold calculation** - off-by-one errors pueden causar data leakage
3. **File locking** - race conditions
4. **Trend detection** - logic compleja con thresholds

**Tests requeridos:**

```python
# tests/test_drift_trends.py
class TestDriftTrendAnalyzer:
    def test_drift_score_calculation_bounds(self):
        """Score should be 0-100."""
        # Test with extreme values
        # Test with zero p-values
        # Test with missing tests

    def test_concurrent_record_drift(self):
        """Multiple processes writing simultaneously."""
        # Use multiprocessing to test race condition

    def test_trend_detection_thresholds(self):
        """Verify trend classification at boundary values."""
        # r2 = 0.29 vs 0.31
        # slope = 0.49 vs 0.51

# tests/test_validation.py
class TestWalkForwardValidator:
    def test_fold_calculation_no_overlap(self):
        """Train and test sets should not overlap."""
        series = pd.Series(range(200))
        validator = WalkForwardValidator(...)

        folds = validator._calculate_folds(series, max_folds=5)

        for train_idx, test_idx in folds:
            # Assert no overlap
            assert len(set(train_idx) & set(test_idx)) == 0

    def test_expanding_vs_rolling(self):
        """Expanding window should grow, rolling should stay constant."""

    def test_insufficient_data_handling(self):
        """Should handle series too short for validation."""

# tests/test_dashboard_utils.py
class TestDashboardUtils:
    def test_get_drift_summary_missing_file(self):
        """Should handle missing drift history gracefully."""

    def test_plot_drift_trend_edge_cases(self):
        """Test with empty data, single point, many points."""

    def test_horizon_validation(self):
        """Should reject invalid horizons."""
```

**Prioridad:** HIGH - estos tests deben existir antes de uso intensivo

---

### 6. Seguridad [üî¥]

#### ‚úÖ Aspectos Positivos:

**No hay secrets hardcoded:**
‚úÖ Settings loaded from config
‚úÖ Paths via `get_settings().data_dir`

**Input sanitization en algunos lugares:**
```python
if len(series) < self.baseline_window + self.test_window:
    logger.warning("Insufficient data")
    return None, None
```

#### üî¥ Issues Criticos:

**CRIT-5: Path traversal en multiples funciones**

Ya cubierto en CRIT-3, pero tambien afecta:
- `get_alert_details()` - horizon parameter
- `get_validation_details()` - horizon parameter
- `get_drift_details()` - horizon parameter

**CRIT-6: No input validation en CLI limits**

**Archivo:** `mlops_dashboard.py:219-299`, `validate_model.py:83-244`

**Problema:**
```python
@app.command()
def validation(
    horizon: str = typer.Option("7d"),
    limit: int = typer.Option(5, "--limit", "-n"),  # No validation
):
    val_data = get_validation_details(horizon, limit)

def get_validation_details(horizon: str, limit: int):
    # limit usado directamente en slicing
    for filepath in summary_files[:limit]:  # limit can be negative!
```

Ataque:
```bash
python mlops_dashboard.py validation --limit -1
# Python slicing con negative index puede causar comportamiento inesperado

python mlops_dashboard.py validation --limit 999999
# Puede intentar cargar demasiados archivos
```

**Solucion:**
```python
def validate_limit(limit: int, max_limit: int = 100) -> int:
    """Validate and clamp limit parameter."""
    if limit < 1:
        raise ValueError(f"Limit must be positive, got {limit}")

    if limit > max_limit:
        logger.warning(f"Limit {limit} exceeds max {max_limit}, clamping")
        return max_limit

    return limit

@app.command()
def validation(
    horizon: str = typer.Option("7d"),
    limit: int = typer.Option(5, "--limit", "-n"),
):
    try:
        horizon = validate_horizon(horizon)
        limit = validate_limit(limit)
    except ValueError as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(1)
```

#### üü° Sugerencias de Mejora:

**Sugerencia #6: Log sensitive operations**

**Archivo:** `drift_trends.py`, `validation.py`

**Actual:** Logging informativo pero no security-aware

**Sugerido:** Agregar audit logging
```python
def record_drift(self, report: DriftReport, horizon: str) -> None:
    # ... existing code ...

    # Security audit log
    logger.info(
        f"[AUDIT] Drift recorded: horizon={horizon}, "
        f"severity={report.severity.value}, "
        f"user={os.getenv('USER', 'unknown')}, "
        f"host={socket.gethostname()}"
    )
```

**Beneficio:** Permite detectar uso anomalo del sistema

---

## üéØ Action Items

### üî¥ Critico (Must Fix antes de uso intensivo):

- [ ] **[CRIT-1]** Implementar file locking para Parquet writes - `drift_trends.py:139`, `validation.py:527`
  - Prioridad: P0
  - Tiempo estimado: 4 horas
  - Impacto: Data corruption en escrituras concurrentes

- [ ] **[CRIT-2]** Agregar limites de resource usage - `validation.py:226`, multiples funciones
  - Prioridad: P0
  - Tiempo estimado: 3 horas
  - Impacto: System crash con inputs grandes

- [ ] **[CRIT-3]** Fix path traversal vulnerability - `dashboard_utils.py`, `mlops_dashboard.py`
  - Prioridad: P0
  - Tiempo estimado: 2 horas
  - Impacto: Security breach, acceso a archivos fuera de scope

- [ ] **[CRIT-4]** Agregar tests para drift_trends y validation - `tests/`
  - Prioridad: P0
  - Tiempo estimado: 8 horas
  - Impacto: Bugs no detectados en produccion

### üü° Importante (Should Fix):

- [ ] **[IMP-1]** Hacer drift score weights configurables - `drift_trends.py:253`
  - Prioridad: P1
  - Tiempo estimado: 2 horas
  - Beneficio: Permite optimizar formula basado en datos

- [ ] **[IMP-2]** Extraer magic numbers a constantes - `drift_trends.py:356`, multiple files
  - Prioridad: P1
  - Tiempo estimado: 1 hora
  - Beneficio: Mas facil ajustar thresholds

- [ ] **[IMP-3]** Refactor get_alert_details (107 lineas) - `dashboard_utils.py:390`
  - Prioridad: P1
  - Tiempo estimado: 3 horas
  - Beneficio: Reduce complejidad, mas testeable

- [ ] **[IMP-4]** Implementar caching para Parquet reads - `dashboard_utils.py`
  - Prioridad: P1
  - Tiempo estimado: 2 horas
  - Beneficio: -80% lecturas de disco

- [ ] **[IMP-5]** Validar todos los CLI inputs - `mlops_dashboard.py`, `validate_model.py`
  - Prioridad: P1
  - Tiempo estimado: 2 horas
  - Beneficio: Previene errores y security issues

- [ ] **[IMP-6]** Mejor logging de archivos faltantes - `dashboard_utils.py:178`
  - Prioridad: P1
  - Tiempo estimado: 1 hora
  - Beneficio: Mejor debugging

### üü¢ Nice-to-Have (Could Fix):

- [ ] **[NTH-1]** Usar SQLite en lugar de Parquet para concurrency - `drift_trends.py`, `validation.py`
  - Prioridad: P2
  - Tiempo estimado: 8 horas
  - Beneficio: Mejor concurrency nativa

- [ ] **[NTH-2]** Agregar progress bars a validation - `validate_model.py`
  - Prioridad: P2
  - Tiempo estimado: 1 hora
  - Beneficio: Better UX

- [ ] **[NTH-3]** Implementar async I/O para dashboard - `dashboard_utils.py`
  - Prioridad: P2
  - Tiempo estimado: 6 horas
  - Beneficio: Faster dashboard rendering

---

## üí° Patrones de Codigo

### ‚úÖ Buenos Patrones a Replicar:

**1. Dataclasses con metodos de negocio:**
```python
@dataclass
class DriftTrendReport:
    trend: DriftTrend
    current_score: float
    # ... fields

    def requires_action(self) -> bool:
        """Business logic on the model itself."""
        return self.trend == DriftTrend.WORSENING or ...
```
**Por que:** Encapsula datos + comportamiento relacionado

**2. Factory pattern para reportes vacios:**
```python
def _create_empty_report(self) -> DriftTrendReport:
    """Crea reporte vacio cuando no hay datos suficientes."""
    return DriftTrendReport(
        trend=DriftTrend.UNKNOWN,
        current_score=0.0,
        # ... defaults
    )
```
**Por que:** Siempre retorna objeto valido, nunca None

**3. Enums con str base para serializacion:**
```python
class DriftTrend(str, Enum):
    STABLE = "stable"
```
**Por que:** Serializable a JSON, comparable con strings

**4. Dependency injection con defaults:**
```python
def __init__(
    self,
    storage_path: Optional[Path] = None,
    drift_detector: Optional[DataDriftDetector] = None,
):
    self.storage_path = storage_path or self._get_default_path()
    self.drift_detector = drift_detector or DataDriftDetector()
```
**Por que:** Testeable pero conveniente en produccion

### ‚ùå Patrones a Evitar:

**1. Read-modify-write sin locking:**
```python
# BAD
df = pd.read_parquet(path)
df = pd.concat([df, new_data])
df.to_parquet(path)  # Race condition!
```

**2. User input directo en file paths:**
```python
# BAD
horizon = user_input
pattern = f"summary_{horizon}_*.parquet"  # Path traversal!
```

**3. Funciones >100 lineas con nested loops:**
```python
# BAD - get_alert_details tiene 107 lineas con 3 nested loops
def get_alert_details(...):
    for h in horizons:
        for block in content.split():
            for line in block.split():
                if line.startswith(...):
```

**4. Magic numbers sin documentacion:**
```python
# BAD
if r2 < 0.3:  # What is 0.3? Why not 0.25 or 0.35?
```

---

## üß™ Testing Recommendations

### Tests Criticos Faltantes:

**1. Drift Trends Module:**
```python
# tests/test_drift_trends.py

def test_drift_score_bounds():
    """Score must be 0-100."""

def test_drift_score_weights_sum_to_one():
    """Weights must sum to 1.0."""

def test_concurrent_drift_recording():
    """Multiple processes writing should not corrupt data."""

def test_trend_detection_boundary_values():
    """Test r2=0.3, slope=0.5 boundaries."""

def test_empty_history_handling():
    """Should return UNKNOWN trend for empty history."""
```

**2. Validation Module:**
```python
# tests/test_validation.py

def test_fold_calculation_no_data_leakage():
    """Train and test sets must not overlap."""

def test_expanding_window_grows():
    """Expanding mode: train size increases each fold."""

def test_rolling_window_constant():
    """Rolling mode: train size stays constant."""

def test_validation_with_mock_forecaster():
    """Test full validation pipeline with deterministic forecaster."""

def test_insufficient_data_returns_empty_report():
    """Should handle series too short gracefully."""
```

**3. Dashboard Utils:**
```python
# tests/test_dashboard_utils.py

def test_get_drift_summary_missing_file():
    """Should return empty list for missing file."""

def test_plot_drift_trend_empty_data():
    """Should handle empty history."""

def test_horizon_validation_rejects_invalid():
    """Should reject '../../../etc/passwd'."""

def test_limit_validation_clamps_negative():
    """Should reject negative limits."""
```

**4. Integration Tests:**
```python
# tests/test_mlops_integration.py

def test_full_drift_trend_pipeline():
    """Record drift -> analyze trend -> generate report."""

def test_full_validation_pipeline():
    """Load data -> validate -> save report."""

def test_dashboard_all_commands():
    """Test CLI commands don't crash."""
```

### Test Coverage Target:

- **Critical paths:** 90%+ coverage
  - Drift score calculation
  - Fold calculation
  - File I/O operations

- **Business logic:** 80%+ coverage
  - Trend detection
  - Severity classification
  - Recommendations

- **Utils/helpers:** 70%+ coverage

---

## üîß Refactoring Opportunities

### 1. Consolidar storage operations

**Problema:** Parquet read/write duplicado en 3 lugares
- `drift_trends.py`
- `validation.py`
- `dashboard_utils.py` (implicit via analyzers)

**Solucion:** Storage abstraction layer

```python
# src/forex_core/mlops/storage.py

from abc import ABC, abstractmethod
from pathlib import Path
from typing import Protocol
import pandas as pd

class MLOpsStorage(Protocol):
    """Protocol for MLOps storage backends."""

    def read(self, key: str) -> pd.DataFrame:
        ...

    def write(self, key: str, df: pd.DataFrame) -> None:
        ...

    def append(self, key: str, record: dict) -> None:
        ...

class ParquetStorage:
    """Thread-safe Parquet storage with file locking."""

    def __init__(self, base_path: Path):
        self.base_path = base_path
        self._locks = {}

    @contextmanager
    def _lock(self, key: str):
        """Acquire lock for key."""
        lock_path = self.base_path / f"{key}.lock"
        with open(lock_path, 'w') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                yield
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)

    def read(self, key: str) -> pd.DataFrame:
        path = self.base_path / f"{key}.parquet"
        if not path.exists():
            return pd.DataFrame()
        return pd.read_parquet(path)

    def write(self, key: str, df: pd.DataFrame) -> None:
        with self._lock(key):
            path = self.base_path / f"{key}.parquet"
            temp_path = path.with_suffix('.tmp')
            df.to_parquet(temp_path, index=False)
            temp_path.replace(path)  # Atomic

    def append(self, key: str, record: dict) -> None:
        with self._lock(key):
            df = self.read(key)
            df = pd.concat([df, pd.DataFrame([record])], ignore_index=True)
            self.write(key, df)

# Usage in DriftTrendAnalyzer
class DriftTrendAnalyzer:
    def __init__(
        self,
        storage: Optional[MLOpsStorage] = None,
    ):
        self.storage = storage or ParquetStorage(Path("data/drift_history"))

    def record_drift(self, report: DriftReport, horizon: str):
        record = {...}
        self.storage.append("drift_history", record)
```

**Beneficio:**
- File locking centralizado
- Facil cambiar a SQLite/Postgres
- Mejor testeable con mock storage

### 2. Extraer parsing logic de dashboard_utils

**Problema:** Log parsing embebido en `get_alert_details()`

**Solucion:** Alert log parser class

```python
# src/forex_core/mlops/alert_parser.py

@dataclass
class AlertEvent:
    timestamp: datetime
    horizon: str
    decision: str
    severity: str
    reason: str

class AlertLogParser:
    """Parser for AlertManager log files."""

    BLOCK_SEPARATOR = "=" * 60

    def parse_file(self, log_file: Path) -> list[AlertEvent]:
        """Parse entire log file."""
        with open(log_file) as f:
            content = f.read()

        return [
            event
            for block in content.split(self.BLOCK_SEPARATOR)
            if (event := self._parse_block(block)) is not None
        ]

    def _parse_block(self, block: str) -> Optional[AlertEvent]:
        """Parse single alert block."""
        if "ALERT EVALUATION" not in block:
            return None

        fields = {}
        for line in block.split("\n"):
            if ":" in line:
                key, value = line.split(":", 1)
                fields[key.strip().lower()] = value.strip()

        try:
            return AlertEvent(
                timestamp=datetime.strptime(fields["timestamp"], "%Y-%m-%d %H:%M:%S"),
                horizon=fields.get("horizon", "unknown"),
                decision=fields.get("decision", ""),
                severity=fields.get("severity", ""),
                reason=fields.get("reason", ""),
            )
        except (KeyError, ValueError) as e:
            logger.warning(f"Failed to parse alert block: {e}")
            return None

# Usage in dashboard_utils
def get_alert_details(horizon, days, severity):
    parser = AlertLogParser()

    events = []
    for h in horizons:
        log_file = logs_dir / f"alerts_{h}.log"
        if log_file.exists():
            events.extend(parser.parse_file(log_file))

    # Filter by cutoff date and severity
    cutoff = datetime.now() - timedelta(days=days)
    filtered = [
        e for e in events
        if e.timestamp >= cutoff
        and (not severity or severity.lower() in e.severity.lower())
    ]

    return format_alert_summary(filtered)
```

**Beneficio:**
- Testeable independientemente
- Reutilizable en otros contextos
- Mas facil de mantener

### 3. Configuration management

**Problema:** Thresholds y settings dispersos en codigo

**Solucion:** Centralized MLOps config

```python
# src/forex_core/mlops/config.py

from dataclasses import dataclass
from pathlib import Path

@dataclass
class DriftTrendConfig:
    """Configuration for drift trend analysis."""
    r2_threshold_weak: float = 0.3
    score_threshold_stable: float = 25.0
    slope_threshold_flat: float = 0.5
    consecutive_high_threshold: int = 3
    critical_score_threshold: float = 75.0

@dataclass
class DriftScoreWeights:
    """Weights for drift score calculation."""
    ks_weight: float = 0.40
    t_test_weight: float = 0.25
    levene_weight: float = 0.20
    ljungbox_weight: float = 0.15

    def __post_init__(self):
        total = sum([self.ks_weight, self.t_test_weight,
                     self.levene_weight, self.ljungbox_weight])
        if not np.isclose(total, 1.0):
            raise ValueError(f"Weights must sum to 1.0, got {total}")

@dataclass
class ValidationConfig:
    """Configuration for walk-forward validation."""
    max_folds: int = 20
    max_series_length: int = 5000
    max_forecast_horizon: int = 365
    default_initial_train_days: int = 365
    default_test_days: int = 30
    default_step_days: int = 30

@dataclass
class MLOpsConfig:
    """Master MLOps configuration."""
    drift_trends: DriftTrendConfig
    drift_scores: DriftScoreWeights
    validation: ValidationConfig
    storage_base_path: Path
    max_parquet_size_mb: int = 100
    cache_ttl_seconds: int = 60

    @classmethod
    def from_env(cls) -> "MLOpsConfig":
        """Load config from environment/settings."""
        from forex_core.config import get_settings
        settings = get_settings()

        return cls(
            drift_trends=DriftTrendConfig(),
            drift_scores=DriftScoreWeights(),
            validation=ValidationConfig(),
            storage_base_path=settings.data_dir,
        )

# Usage
config = MLOpsConfig.from_env()
analyzer = DriftTrendAnalyzer(config=config)
```

**Beneficio:**
- Single source of truth
- Facil override para testing
- Documentado via dataclass

---

## üìö Preocupaciones Especificas

### 1. Storage Strategy: Parquet vs Alternatives

**Parquet Analysis:**

‚úÖ **Pros:**
- Columnar storage = rapido para analytics
- Buena compresion (50-80% vs CSV)
- Schema enforcement
- Compatible con pandas, DuckDB, Polars

‚ùå **Cons:**
- No concurrent writes (sin external locking)
- Read-modify-write entire file para append
- No transactions
- No indices para queries complejas

**Alternativa 1: SQLite with WAL mode**

```python
import sqlite3

# Enable WAL mode for better concurrency
conn = sqlite3.connect('drift_history.db')
conn.execute("PRAGMA journal_mode=WAL")

# WAL allows:
# - Multiple concurrent readers
# - 1 concurrent writer
# - Better crash recovery
```

‚úÖ **Pros:**
- ACID transactions
- Concurrent reads + writes
- Indices para queries rapidas
- No external locking needed

‚ùå **Cons:**
- Slightly slower than Parquet para bulk analytics
- Mas complejo setup

**Recomendacion:**
- **Keep Parquet** para datos historicos (read-only analytics)
- **Use SQLite** para datos activos (drift recording, tracking)

```python
# Hybrid approach
class MLOpsStorage:
    def __init__(self):
        self.sqlite = sqlite3.connect('active_data.db')
        self.parquet_dir = Path('historical_data')

    def append(self, table: str, record: dict):
        # Write to SQLite (concurrent-safe)
        self.sqlite.execute(f"INSERT INTO {table} VALUES (?...)", record)

    def archive_old_data(self, cutoff_date: datetime):
        # Move old SQLite data to Parquet
        old_data = pd.read_sql(f"SELECT * FROM drift WHERE date < ?",
                               self.sqlite, params=[cutoff_date])
        old_data.to_parquet(self.parquet_dir / f"drift_{cutoff_date}.parquet")
        self.sqlite.execute("DELETE FROM drift WHERE date < ?", [cutoff_date])
```

### 2. Drift Scoring Formula

**Current Formula:**
```python
score = (KS * 0.40) + (T-test * 0.25) + (Levene * 0.20) + (LB * 0.15)
```

**Analysis:**

‚úÖ **Justificacion actual es razonable:**
- KS (40%): Distribution change es mas importante
- T-test (25%): Mean shift es segundo mas importante
- Levene (20%): Variance change importante
- Ljung-Box (15%): Autocorrelation menos critico

‚ùå **Problemas:**
1. No hay validacion empirica de estos weights
2. Hardcoded = no puede optimizar
3. No considera context (e.g., forex vs otros assets)

**Recomendaciones:**

**A. Hacer weights configurables** (ya cubierto en IMP-1)

**B. Validar empiricamente con backtesting:**
```python
# Experiment framework
def evaluate_drift_weights(
    historical_data: pd.Series,
    retraining_events: list[datetime],
    weights: DriftScoreWeights,
) -> float:
    """
    Evaluate drift weights by seeing how well they predict
    when retraining was actually needed.

    Returns:
        F1 score for detecting retraining events.
    """
    analyzer = DriftTrendAnalyzer(score_weights=weights)

    predictions = []
    actuals = []

    for date in historical_data.index:
        report = analyzer.analyze_trend_at_date(date)
        predictions.append(report.requires_action())
        actuals.append(date in retraining_events)

    # Calculate F1 score
    from sklearn.metrics import f1_score
    return f1_score(actuals, predictions)

# Grid search over weights
best_weights = None
best_f1 = 0

for ks_w in np.arange(0.3, 0.5, 0.05):
    for t_w in np.arange(0.2, 0.3, 0.05):
        # ... etc
        weights = DriftScoreWeights(ks_w, t_w, ...)
        f1 = evaluate_drift_weights(data, events, weights)

        if f1 > best_f1:
            best_f1 = f1
            best_weights = weights
```

**C. Considerar ML-based drift scoring:**
```python
# Train classifier to predict "retraining needed"
# Features: KS stat, t-stat, levene stat, LB stat, ...
# Target: whether model performance degraded

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier()
clf.fit(X_train, y_train)

# Feature importance tells us optimal weights
feature_importance = clf.feature_importances_
```

### 3. Validation Data Leakage Risk

**Problema Identificado:**

```python
# validation.py:402-431
def _create_bundle(self, series: pd.Series) -> DataBundle:
    """Creates MOCK bundle with empty features!"""
    return DataBundle(
        usdclp_series=series,
        copper_series=pd.Series(dtype=float),  # EMPTY!
        tpm_series=pd.Series(dtype=float),     # EMPTY!
        # ... all other features empty
    )
```

**Riesgo:** Si el forecaster real usa features (copper, TPM, etc),
la validacion con mock bundle no refleja performance real.

**Analisis:**

1. **Si forecaster es univariate (solo USD/CLP):** ‚úÖ OK
2. **Si forecaster es multivariate:** ‚ùå Validacion incorrecta

**Solucion:**

```python
def _create_bundle(self, series: pd.Series) -> DataBundle:
    """
    Create bundle with historical features for validation.

    IMPORTANT: Must load actual historical features (copper, TPM, etc)
    for the training period, otherwise validation is unrealistic.
    """
    # Load historical data for this period
    from forex_core.data.loader import DataLoader

    loader = DataLoader()

    # Load full bundle up to series end date
    end_date = series.index[-1]
    full_bundle = loader.load(end_date=end_date)

    # Filter to match training series dates
    start_date = series.index[0]

    # Slice all features to match series period
    return DataBundle(
        usdclp_series=series,
        copper_series=full_bundle.copper_series[start_date:end_date],
        tpm_series=full_bundle.tpm_series[start_date:end_date],
        inflation_series=full_bundle.inflation_series[start_date:end_date],
        # ... all features properly sliced
        indicators=full_bundle.indicators,
        macro_events=full_bundle.macro_events,
        news=full_bundle.news,
        sources=full_bundle.sources,
    )
```

**Warning en docstring:**
```python
class WalkForwardValidator:
    """
    ...

    IMPORTANT: The forecaster_func will receive a DataBundle with
    historical features loaded for the training period. Ensure your
    DataLoader can handle point-in-time loading to avoid lookahead bias.

    Example of CORRECT forecaster:
        def my_forecaster(bundle: DataBundle, horizon: int) -> ForecastPackage:
            # Bundle only contains data up to training end date
            # No future information leaked
            model = train_model(bundle)
            return model.forecast(horizon)

    Example of INCORRECT forecaster:
        def bad_forecaster(bundle: DataBundle, horizon: int) -> ForecastPackage:
            # BAD: Calling external API that returns current data
            current_copper = fetch_copper_price()  # Lookahead bias!
    """
```

### 4. Dashboard Performance con Datasets Grandes

**Problema:** Dashboard commands pueden ser lentos

**Analisis:**

```python
# Worst case:
# - drift_history.parquet: 10,000 rows (years of data)
# - validation reports: 100 files
# - alert logs: 1MB each

def show():
    pred_summary = get_prediction_summary()    # Read 1 parquet
    drift_summary = get_drift_summary()        # Read 1 parquet + analyze
    val_summary = get_validation_summary()     # Glob + read N parquets
    alert_summary = get_alert_summary(days=7)  # Read 4 log files
```

**Mediciones estimadas (sin optimizacion):**
- `get_prediction_summary()`: ~100ms
- `get_drift_summary()`: ~500ms (analyze_trend es costoso)
- `get_validation_summary()`: ~300ms
- `get_alert_summary()`: ~200ms
- **Total: ~1.1 segundos**

Con cache (IMP-4): **~200ms** ‚úÖ

**Recomendacion Adicional: Pre-computed summaries**

```python
# Background job (cron) que actualiza summaries cada 5 min
# scripts/update_dashboard_cache.py

def update_dashboard_cache():
    """Pre-compute dashboard summaries."""
    cache_file = Path("data/dashboard_cache.json")

    summaries = {
        "predictions": get_prediction_summary(),
        "drift": get_drift_summary(),
        "validation": get_validation_summary(),
        "alerts": get_alert_summary(days=7),
        "readiness": get_readiness_summary(),
        "updated_at": datetime.now().isoformat(),
    }

    with open(cache_file, 'w') as f:
        json.dump(summaries, f)

# Dashboard reads from cache
def show():
    cache_file = Path("data/dashboard_cache.json")

    if cache_file.exists():
        with open(cache_file) as f:
            data = json.load(f)

        age = (datetime.now() - datetime.fromisoformat(data["updated_at"])).seconds

        if age < 300:  # 5 minutes
            console.print(f"[dim]Cache age: {age}s[/dim]")
            display_cached_summaries(data)
            return

    # Fallback to live computation
    display_live_summaries()
```

---

## üèÅ Conclusion y Siguiente Paso

### Resumen:

Este es un **proyecto MLOps bien arquitecturado** con:
- ‚úÖ Separacion de responsabilidades clara
- ‚úÖ Documentacion exhaustiva
- ‚úÖ Type hints completos
- ‚úÖ Patrones de diseno solidos

**Sin embargo**, hay **issues criticos de produccion**:
- üî¥ Concurrency no manejada (data corruption risk)
- üî¥ Path traversal vulnerability (security risk)
- üî¥ Resource exhaustion possible (availability risk)
- üî¥ Tests insuficientes (reliability risk)

### Recomendacion: **APPROVE WITH CRITICAL FIXES**

**Decision:** APPROVE pero con condiciones

**Siguiente paso:**

1. **Semana 1 (Critico):**
   - Fix CRIT-1: File locking
   - Fix CRIT-3: Path traversal
   - Fix CRIT-2: Resource limits

2. **Semana 2 (Tests):**
   - Add CRIT-4: Core tests
   - Target: 70% coverage en modulos criticos

3. **Semana 3 (Mejoras):**
   - IMP-1: Configurable weights
   - IMP-4: Caching layer
   - IMP-5: Input validation

**Tiempo total estimado fixes criticos:** ~12 horas

**Requiere re-review despues de fixes:** Si (para validar file locking)

**Puede deployer a produccion?**
- **Ahora:** üü° Si, pero con uso limitado (1 proceso, sin cron jobs concurrentes)
- **Despues fixes:** üü¢ Si, listo para uso intensivo

---

**üìù Generado por:** Code Reviewer Agent
**ü§ñ Claude Code Sonnet 4.5**
**‚è±Ô∏è Tiempo de review:** ~30 minutos
